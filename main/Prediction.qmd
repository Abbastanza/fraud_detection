---
title: "Vorhersage (Klassifikation) betrügerischer Kontotransaktionen"
subtitle: "Solution Engineering in R"
author: "Daniel Borsos, Valerie Högerle, Michaela Hubweber, Florian Ye"
date: today
embed-resources: true
format:
  revealjs:
    scrollable: true
    smaller: true
    theme: solarized ##https://quarto.org/docs/presentations/revealjs/themes.html
    slide-level: 2
fig-align: center
execute: 
  warning: false
---

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(naivebayes)

options(scipen=999)
```

# Milestone 3: Modellierung
```{r}
# Read data
#path = "../data/Fraud_processed.csv"
data <- read.csv("/Users/florianye/Google Drive/My Drive/Files/FH Technikum/1 - Aktuell/Solution Engineering - R/main/Fraud_processed.csv")
# Show first rows of data
head(data)
```

## Train-Test Split {.scrollable}

```{r}
# Change the type of the isFraud and flagFraud column to a factor
data$isFraud <- as.factor(data$isFraud)
data$flagFraud <- as.factor(data$flagFraud)
```


```{r}
### Split data into training and test data ###
# Create a column for the interaction between type and isFraud
data$type_fraud <- interaction(data$type, data$isFraud)

# Stratified allocation based on the combined column
set.seed(10)
splitIndex <- createDataPartition(data$type_fraud, p = 0.75, list = FALSE, times = 1)

# Remove the combined column
data$type_fraud <- NULL
# Split the data into training and test data
fullTrainData <- data[splitIndex, ]
testData <- data[-splitIndex, ]


### Split the training data into training and validation data ####
fullTrainData$type_fraud <- interaction(fullTrainData$type, fullTrainData$isFraud)

# Stratified allocation based on the isFraud column
set.seed(10)
trainSplitIndex <- createDataPartition(fullTrainData$type_fraud, p = 0.75, list = FALSE, times = 1)

# Remove the combined column
fullTrainData$type_fraud <- NULL
# Split the training data into training and validation data
trainData <- fullTrainData[trainSplitIndex, ]
valData <- fullTrainData[-trainSplitIndex, ]
```
```{r}
# Check the distribution of the target variable in the training and test data
cat("Verteilung von 'isFraud' im Trainingsdatensatz:")
round(prop.table(table(trainData$isFraud)), 4)
cat("\nVerteilung von 'isFraud' im Testdatensatz:")
round(prop.table(table(testData$isFraud)), 4)

# Check the distribution of the type and target variable in the training and test data
cat("\nVerteilung der Variablen 'type' und 'isFraud' im Trainingsdatensatz:")
round(prop.table(table(trainData$type, trainData$isFraud)), 5)
cat("\nVerteilung der Variablen 'type' und 'isFraud' im Testdatensatz:")
round(prop.table(table(testData$type, testData$isFraud)), 5)

```

```{r}
# Save the training and test data as csv
#write.csv(trainData, "Fraud_train.csv", row.names = FALSE)
#write.csv(testData, "Fraud_test.csv", row.names = FALSE)
```

## Baseline Modell {.scrollable}

```{r}
# Baseline-Model
baseline_model <- function(data) {
  rep(1, nrow(data))
}
```

```{r}
# Definiere eine Funktion, um die Metriken für das Modell zu berechnen und auszugeben
get_model_metrics <- function(predictions_prob, actual_factor, threshold = 0.5) {
  
  # Klassifiziere basierend auf dem Schwellenwert
  predictions_factor <- factor(ifelse(predictions_prob > threshold, "Yes", "No"), levels = c("No", "Yes"))

  # Berechnung der Konfusionsmatrix
  confusionMatrix <- confusionMatrix(predictions_factor, actual_factor, positive="Yes")
  
  # Ausgabe der Konfusionsmatrix
  print("Konfusionsmatrix:")
  print(confusionMatrix$table)
  
  # Berechnung von Recall und Precision
  recall <- confusionMatrix$byClass['Sensitivity']
  precision <- confusionMatrix$byClass['Precision']
  
  # Berechnung des F1-Score
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  # Vorbereitung der Daten für die AUC-Berechnung
  roc_curve <- roc(actual_factor, predictions_prob)
  auc_score <- auc(roc_curve)
  
  # Ausgabe der Metriken
  print(paste("Recall:", recall))
  print(paste("Precision:", precision))
  print(paste("F1-Score:", f1_score))
  print(paste("AUC Score:", auc_score))
}

evaluate_model_performance <- function(model, test_data, threshold = 0.5) {
  # Vorhersagen mit dem Modell für das Validierungsset als Wahrscheinlichkeiten
  test_predictions_prob <- predict(model, newdata = test_data, type = "prob")[, "Yes"]
  
  # Umwandeln der tatsächlichen Werte in Faktoren
  actual_factor <- factor(test_data$isFraud, levels = c("No", "Yes"))
  
  # Anwendung der Funktion, um die Metriken zu berechnen und auszugeben
  get_model_metrics(test_predictions_prob, actual_factor, threshold)
}
```


```{r}
#baseline_predictions_factor <- factor(baseline_model(valData), levels = c("No", "Yes"))
baseline_predictions_prob <- baseline_model(valData)
actual_factor <- factor(valData$isFraud, levels = c("No", "Yes"))

# Aufruf der Funktion mit den umgewandelten Daten
get_model_metrics(baseline_predictions_prob, actual_factor)
```

## Modell Training {.scrollable}

### Decision Tree
```{r}
# Festlegung von Gewichten für die Fälle
case_weights <- rep(1, nrow(trainData))
case_weights[trainData$isFraud == "Yes"] <- 100  # mehr Gewicht für Betrugsfälle

# Trainiere ein Entscheidungsbaummodell
decision_tree_model <- rpart(isFraud ~ ., data = trainData, method = "class", weights = case_weights)

# Visualisierung des trainierten Modells
rpart.plot(decision_tree_model, main = "Decision Tree Model")
```

```{r}
evaluate_model_performance(decision_tree_model, valData)
```

### Logistische Regression 
```{r}
# Training des logistischen Regressionsmodells
logisticModel <- glm(isFraud ~ ., family = binomial(link = "logit"), data = trainData)
```
```{r}
logistic_predictions_prob <- predict(logisticModel, newdata = valData, type = "response")
get_model_metrics(logistic_predictions_prob, actual_factor, threshold = 0.3)
```

### Naive Bayes
```{r}
# Training des Naive Bayes Modells
naiveBayesModel <- naive_bayes(isFraud ~ ., data = trainData, laplace = 1)
```

```{r}
evaluate_model_performance(naiveBayesModel, valData)
```


