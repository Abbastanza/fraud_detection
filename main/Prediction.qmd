---
title: "Vorhersage (Klassifikation) betrügerischer Kontotransaktionen"
subtitle: "Solution Engineering in R"
author: "Daniel Borsos, Valerie Högerle, Michaela Hubweber, Florian Ye"
date: today
embed-resources: true
format:
  revealjs:
    scrollable: true
    smaller: true
    theme: solarized ##https://quarto.org/docs/presentations/revealjs/themes.html
    slide-level: 2
fig-align: center
execute: 
  warning: false
---

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(naivebayes)

options(scipen=999)
```

# Milestone 3: Modellierung
```{r}
# Read data
#path = "../data/Fraud_processed.csv"
data.full <- read.csv("/Users/florianye/Google Drive/My Drive/Files/FH Technikum/1 - Aktuell/Solution Engineering - R/main/Fraud_processed.csv")
# Show first rows of data
head(data.full)
```

```{r}
# Select columns for modeling
model_vars <- c("type", "amount", "oldbalanceOrg", "newbalanceOrig", "change.balanceOrg", "nameDestType", "flagFraud", "isFraud")
data <- data.full[model_vars]
```


## Train-Test Split {.scrollable}

```{r}
# Change categorical to factor
data$isFraud <- as.factor(data$isFraud)
#data$flagFraud <- as.factor(data$flagFraud)
data$type <- as.factor(data$type)
#data$nameDestType <- as.factor(data$nameDestType)
```


```{r}
### Split data into training and test data ###
# Create a column for the interaction between type and isFraud
data$type_fraud <- interaction(data$type, data$isFraud)

# Stratified allocation based on the combined column
set.seed(10)
splitIndex <- createDataPartition(data$type_fraud, p = 0.75, list = FALSE, times = 1)

# Remove the combined column
data$type_fraud <- NULL
# Split the data into training and test data
fullTrainData <- data[splitIndex, ]
testData <- data[-splitIndex, ]


### Split the training data into training and validation data ####
fullTrainData$type_fraud <- interaction(fullTrainData$type, fullTrainData$isFraud)

# Stratified allocation based on the isFraud column
set.seed(10)
trainSplitIndex <- createDataPartition(fullTrainData$type_fraud, p = 0.75, list = FALSE, times = 1)

# Remove the combined column
fullTrainData$type_fraud <- NULL
# Split the training data into training and validation data
trainData <- fullTrainData[trainSplitIndex, ]
valData <- fullTrainData[-trainSplitIndex, ]
```

```{r}
# Check the distribution of the target variable in the training and test data
cat("Verteilung von 'isFraud' im Trainingsdatensatz:")
round(prop.table(table(trainData$isFraud)), 4)
cat("\nVerteilung von 'isFraud' im Testdatensatz:")
round(prop.table(table(testData$isFraud)), 4)

# Check the distribution of the type and target variable in the training and test data
cat("\nVerteilung der Variablen 'type' und 'isFraud' im Trainingsdatensatz:")
round(prop.table(table(trainData$type, trainData$isFraud)), 5)
cat("\nVerteilung der Variablen 'type' und 'isFraud' im Testdatensatz:")
round(prop.table(table(testData$type, testData$isFraud)), 5)

```

```{r}
# Save the training and test data as csv
#write.csv(trainData, "Fraud_train.csv", row.names = FALSE)
#write.csv(testData, "Fraud_test.csv", row.names = FALSE)
```

## Baseline Modell {.scrollable}

```{r}
# Baseline-Model
baseline_model <- function(data) {
  rep(1, nrow(data))
}
```

```{r}
# Definiere eine Funktion, um die Metriken für das Modell zu berechnen und auszugeben
get_model_metrics <- function(predictions_prob, actual_factor, threshold = 0.5) {
  
  # Klassifiziere basierend auf dem Schwellenwert
  predictions_factor <- factor(ifelse(predictions_prob > threshold, "Yes", "No"), levels = c("No", "Yes"))

  # Berechnung der Konfusionsmatrix
  confusionMatrix <- confusionMatrix(predictions_factor, actual_factor, positive="Yes")
  
  # Ausgabe der Konfusionsmatrix
  print("Konfusionsmatrix:")
  print(confusionMatrix$table)
  
  # Berechnung von Recall und Precision
  recall <- confusionMatrix$byClass['Sensitivity']
  precision <- confusionMatrix$byClass['Precision']
  
  # Berechnung des F1-Score
  f1_score <- 2 * (precision * recall) / (precision + recall)
  
  # Vorbereitung der Daten für die AUC-Berechnung
  roc_curve <- roc(actual_factor, predictions_prob)
  auc_score <- auc(roc_curve)
  
  # Ausgabe der Metriken
  print(paste("Recall:", recall))
  print(paste("Precision:", precision))
  print(paste("F1-Score:", f1_score))
  print(paste("AUC Score:", auc_score))
}

evaluate_model_performance <- function(model, test_data, threshold = 0.5) {
  # Vorhersagen mit dem Modell für das Validierungsset als Wahrscheinlichkeiten
  test_predictions_prob <- predict(model, newdata = test_data, type = "prob")[, "Yes"]
  
  # Umwandeln der tatsächlichen Werte in Faktoren
  actual_factor <- factor(test_data$isFraud, levels = c("No", "Yes"))
  
  # Anwendung der Funktion, um die Metriken zu berechnen und auszugeben
  get_model_metrics(test_predictions_prob, actual_factor, threshold)
}
```


```{r}
#baseline_predictions_factor <- factor(baseline_model(valData), levels = c("No", "Yes"))
baseline_predictions_prob <- baseline_model(valData)
actual_factor <- factor(valData$isFraud, levels = c("No", "Yes"))

# Aufruf der Funktion mit den umgewandelten Daten
get_model_metrics(baseline_predictions_prob, actual_factor)
```

## Modell Training {.scrollable}
Die Modelle werden hier vorerst mit einem Teil der Trainingsdaten trainiert und mit dem Validierungsdatensatz evaluiert.
### Decision Tree
```{r}
# Festlegung von Gewichten für die Fälle
case_weights <- rep(1, nrow(trainData))
case_weights[trainData$isFraud == "Yes"] <- 100  # mehr Gewicht für Betrugsfälle

# Trainiere ein Entscheidungsbaummodell
decision_tree_model <- rpart(isFraud ~ ., data = trainData, method = "class", weights = case_weights)

# Visualisierung des trainierten Modells
rpart.plot(decision_tree_model, main = "Decision Tree Model")
```

```{r}
evaluate_model_performance(decision_tree_model, valData)
```

### Logistische Regression 
```{r}
# Training des logistischen Regressionsmodells
logisticModel <- glm(isFraud ~ ., family = binomial(link = "logit"), data = trainData)
```

```{r}
logistic_predictions_prob <- predict(logisticModel, newdata = valData, type = "response")
get_model_metrics(logistic_predictions_prob, actual_factor, threshold = 0.5)
```

### Naive Bayes
```{r}
# Training des Naive Bayes Modells
naiveBayesModel <- naive_bayes(isFraud ~ ., data = trainData, laplace = 1)
```

```{r}
evaluate_model_performance(naiveBayesModel, valData)
```

Alle 3 Modelle zeigen bessere Ergebnisse als das Baseline-Modell. Das Decision Tree und Logistic Regression Modell haben sehr gute und ähnliche Ergebnisse, während das Naive Bayes Modell etwas schlechter abschneidet. Im nächsten Schritt werden die Modelle mit dem gesamten Trainingsdatensatz trainiert und mit dem Testdatensatz evaluiert.

## Modell Evaluation {.scrollable}
```{r}
# Decision Tree
case_weights_full <- rep(1, nrow(fullTrainData)) # Festlegung von Gewichten für die Fälle
case_weights_full[fullTrainData$isFraud == "Yes"] <- 100  # mehr Gewicht für Betrugsfälle
decision_tree_model_full <- rpart(isFraud ~ ., data = fullTrainData, method = "class", weights = case_weights_full)
evaluate_model_performance(decision_tree_model_full, testData)
```

```{r}
# Logistic Regression
actual_factor_test <- factor(testData$isFraud, levels = c("No", "Yes"))

logisticModel_full <- glm(isFraud ~ ., family = binomial(link = "logit"), data = fullTrainData)
logistic_predictions_prob_full <- predict(logisticModel_full, newdata = testData, type = "response")
get_model_metrics(logistic_predictions_prob_full, actual_factor_test, threshold = 0.5)
```

```{r}
# Naive Bayes
naiveBayesModel_full <- naive_bayes(isFraud ~ ., data = fullTrainData, laplace = 1)
evaluate_model_performance(naiveBayesModel_full, testData)
```

Wie auch beim Trainings und Validierungsdatensatz, zeigen das Decision Tree und Logistic Regression Modell die besten Ergebnisse. Das Naive Bayes Modell schneidet wieder etwas schlechter ab. Die Modelle wurden mit dem gesamten Trainingsdatensatz trainiert und mit dem Testdatensatz evaluiert. Da sich die Ergebnisse der Modelle nicht wesentlich unterscheiden, wird das Decision Tree Modell für die Vorhersage der Testdaten verwendet.

```{r}
# Speichern des Decision Tree Modells
saveRDS(decision_tree_model_full, "decision_tree_model.rds")
```



